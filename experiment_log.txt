model = CBERT(vocab_size=len(id_to_char),
                 embed_size=768,
                 dim_feedforward=2048,
                 num_heads=12,
                 num_layers=6,
                 pad_idx=char_to_id['<PAD>'])

# lr = 1e-5 (result is base.pt)
Epoch:1 Loss:0.061538, TrainAcc:0.208431
Epoch:2 Loss:0.058416, TrainAcc:0.248572
Epoch:3 Loss:0.057905, TrainAcc:0.253689
Epoch:4 Loss:0.057590, TrainAcc:0.257060
Epoch:5 Loss:0.057253, TrainAcc:0.258164
Epoch:6 Loss:0.057058, TrainAcc:0.259192
Epoch:7 Loss:0.056633, TrainAcc:0.262608
Epoch:8 Loss:0.056109, TrainAcc:0.267858
Epoch:9 Loss:0.055933, TrainAcc:0.269792
Epoch:10 Loss:0.055813, TrainAcc:0.270267
Epoch:11 Loss:0.055680, TrainAcc:0.272121
Epoch:12 Loss:0.055508, TrainAcc:0.275259
Epoch:13 Loss:0.055358, TrainAcc:0.278295
Epoch:14 Loss:0.055269, TrainAcc:0.279145
Epoch:15 Loss:0.055161, TrainAcc:0.280366
Epoch:16 Loss:0.055079, TrainAcc:0.281053
Epoch:17 Loss:0.054938, TrainAcc:0.282940
Epoch:18 Loss:0.054869, TrainAcc:0.283184
Epoch:19 Loss:0.054790, TrainAcc:0.284508
Epoch:20 Loss:0.054785, TrainAcc:0.284957
Epoch:21 Loss:0.054699, TrainAcc:0.285782
Epoch:22 Loss:0.054587, TrainAcc:0.287134
Epoch:23 Loss:0.054468, TrainAcc:0.288320
Epoch:24 Loss:0.054431, TrainAcc:0.288946
Epoch:25 Loss:0.054313, TrainAcc:0.290255
Epoch:26 Loss:0.054257, TrainAcc:0.290490
Epoch:27 Loss:0.054182, TrainAcc:0.291619
Epoch:28 Loss:0.054107, TrainAcc:0.292221
Epoch:29 Loss:0.054044, TrainAcc:0.292967
Epoch:30 Loss:0.053997, TrainAcc:0.293373
Epoch:31 Loss:0.053915, TrainAcc:0.294141
Epoch:32 Loss:0.053847, TrainAcc:0.294821
Epoch:33 Loss:0.053767, TrainAcc:0.295561
Epoch:34 Loss:0.053696, TrainAcc:0.296362
Epoch:35 Loss:0.053670, TrainAcc:0.296613
Epoch:36 Loss:0.053574, TrainAcc:0.297502
Epoch:37 Loss:0.053469, TrainAcc:0.299594
Epoch:38 Loss:0.053392, TrainAcc:0.300096
Epoch:39 Loss:0.053284, TrainAcc:0.301653
Epoch:40 Loss:0.053133, TrainAcc:0.303322
Epoch:41 Loss:0.052998, TrainAcc:0.305495
Epoch:42 Loss:0.052867, TrainAcc:0.306919
Epoch:43 Loss:0.052672, TrainAcc:0.308783
Epoch:44 Loss:0.052433, TrainAcc:0.311707
Epoch:45 Loss:0.052196, TrainAcc:0.314220
Epoch:46 Loss:0.051865, TrainAcc:0.318648
Epoch:47 Loss:0.051485, TrainAcc:0.323468
Epoch:48 Loss:0.050976, TrainAcc:0.328343
Epoch:49 Loss:0.050308, TrainAcc:0.335977
Epoch:50 Loss:0.049480, TrainAcc:0.344647

# lr = 1?2? e-5 (result is yelp_cp00.pt)
Epoch:1 Loss:0.048485, TrainAcc:0.355053
Epoch:2 Loss:0.047446, TrainAcc:0.366660
Epoch:3 Loss:0.046438, TrainAcc:0.378269
Epoch:4 Loss:0.045436, TrainAcc:0.389140
Epoch:5 Loss:0.044404, TrainAcc:0.400285
Epoch:6 Loss:0.043316, TrainAcc:0.411271
Epoch:7 Loss:0.042267, TrainAcc:0.422691
Epoch:8 Loss:0.041315, TrainAcc:0.433047
Epoch:9 Loss:0.040441, TrainAcc:0.442482
Epoch:10 Loss:0.039629, TrainAcc:0.451327

# lr = 5e-5 (result is yelp2_cp00.pt / accidently iterated only once)
Training iter 0, lr 5e-05
Epoch:1 Loss:0.037886, TrainAcc:0.470810
Epoch:2 Loss:0.035347, TrainAcc:0.501738
Epoch:3 Loss:0.033342, TrainAcc:0.527803
Epoch:4 Loss:0.031772, TrainAcc:0.549541
Epoch:5 Loss:0.030504, TrainAcc:0.566395
Val_Loss:0.027194, Val_Acc:0.615027

# The whole thing is going so well it scares me

# lr 5e-05 (result is yelp3_cp00.pt / accidently turned off / i'm dumb)
Training iter 0, lr 5e-05
Epoch:1 Loss:0.029453, TrainAcc:0.580152
Epoch:2 Loss:0.028562, TrainAcc:0.591754
Epoch:3 Loss:0.027760, TrainAcc:0.602638
Epoch:4 Loss:0.027040, TrainAcc:0.612160
Epoch:5 Loss:0.026404, TrainAcc:0.621232
Val_Loss:0.023299, Val_Acc:0.667655

# lr 5e-05 (result is yelp4_cp05 ~ 13.pt (00~04 is overwritten by 5th one))
Training iter 0, lr 5e-05
Epoch:1 Loss:0.025881, TrainAcc:0.628679
Epoch:2 Loss:0.025369, TrainAcc:0.635577
Epoch:3 Loss:0.024896, TrainAcc:0.642277
Epoch:4 Loss:0.024394, TrainAcc:0.649133
Epoch:5 Loss:0.024002, TrainAcc:0.654541
Val_Loss:0.020939, Val_Acc:0.701894
Training iter 1, lr 5e-05
Epoch:1 Loss:0.023586, TrainAcc:0.660490
Epoch:2 Loss:0.023218, TrainAcc:0.665409
Epoch:3 Loss:0.022827, TrainAcc:0.671344
Epoch:4 Loss:0.022480, TrainAcc:0.676482
Epoch:5 Loss:0.022171, TrainAcc:0.680836
Val_Loss:0.019443, Val_Acc:0.723848
Training iter 2, lr 5e-05
Epoch:1 Loss:0.021773, TrainAcc:0.686610
Epoch:2 Loss:0.021507, TrainAcc:0.690351
Epoch:3 Loss:0.021231, TrainAcc:0.694481
Epoch:4 Loss:0.021006, TrainAcc:0.697582
Epoch:5 Loss:0.020718, TrainAcc:0.701674
Val_Loss:0.018122, Val_Acc:0.742524
Training iter 3, lr 5e-05
Epoch:1 Loss:0.020379, TrainAcc:0.707301
Epoch:2 Loss:0.020188, TrainAcc:0.709493
Epoch:3 Loss:0.019916, TrainAcc:0.713001
Epoch:4 Loss:0.019702, TrainAcc:0.716182
Epoch:5 Loss:0.019474, TrainAcc:0.719590
Val_Loss:0.016798, Val_Acc:0.760090
Training iter 4, lr 5e-05
Epoch:1 Loss:0.019243, TrainAcc:0.722982
Epoch:2 Loss:0.019061, TrainAcc:0.725590
Epoch:3 Loss:0.018804, TrainAcc:0.729559
Epoch:4 Loss:0.018638, TrainAcc:0.732047
Epoch:5 Loss:0.018434, TrainAcc:0.734685
Val_Loss:0.015943, Val_Acc:0.773545
Training iter 5, lr 5e-05
Epoch:1 Loss:0.018232, TrainAcc:0.737910
Epoch:2 Loss:0.018015, TrainAcc:0.740741
Epoch:3 Loss:0.017848, TrainAcc:0.743335
Epoch:4 Loss:0.017653, TrainAcc:0.746237
Epoch:5 Loss:0.017451, TrainAcc:0.749005
Val_Loss:0.014984, Val_Acc:0.786724
Training iter 6, lr 5e-05
Epoch:1 Loss:0.017256, TrainAcc:0.751644
Epoch:2 Loss:0.017073, TrainAcc:0.754670
Epoch:3 Loss:0.016842, TrainAcc:0.757880
Epoch:4 Loss:0.016711, TrainAcc:0.759824
Epoch:5 Loss:0.016507, TrainAcc:0.762779
Val_Loss:0.014360, Val_Acc:0.797337
Training iter 7, lr 5e-05
Epoch:1 Loss:0.016355, TrainAcc:0.764691
Epoch:2 Loss:0.016185, TrainAcc:0.767348
Epoch:3 Loss:0.015991, TrainAcc:0.770033
Epoch:4 Loss:0.015882, TrainAcc:0.772072
Epoch:5 Loss:0.015684, TrainAcc:0.774625
Val_Loss:0.013541, Val_Acc:0.807135
Training iter 8, lr 5e-05
Epoch:1 Loss:0.015531, TrainAcc:0.776976
Epoch:2 Loss:0.015400, TrainAcc:0.778660
Epoch:3 Loss:0.015262, TrainAcc:0.781332
Epoch:4 Loss:0.015119, TrainAcc:0.782996
Epoch:5 Loss:0.014924, TrainAcc:0.785444
Val_Loss:0.013050, Val_Acc:0.816771
Training iter 9, lr 5e-05
Epoch:1 Loss:0.014794, TrainAcc:0.787247
Epoch:2 Loss:0.014708, TrainAcc:0.788873
Epoch:3 Loss:0.014572, TrainAcc:0.790795
Epoch:4 Loss:0.014425, TrainAcc:0.792460
Epoch:5 Loss:0.014339, TrainAcc:0.794377
Val_Loss:0.012520, Val_Acc:0.825151
Training iter 10, lr 5e-05
Epoch:1 Loss:0.014204, TrainAcc:0.796357
Epoch:2 Loss:0.014147, TrainAcc:0.797190
Epoch:3 Loss:0.013983, TrainAcc:0.799670
Epoch:4 Loss:0.013876, TrainAcc:0.801139
Epoch:5 Loss:0.013832, TrainAcc:0.801786
Val_Loss:0.012111, Val_Acc:0.831311
Training iter 11, lr 5e-05
Epoch:1 Loss:0.013672, TrainAcc:0.803618
Epoch:2 Loss:0.013527, TrainAcc:0.805851
Epoch:3 Loss:0.013455, TrainAcc:0.806991
Epoch:4 Loss:0.013342, TrainAcc:0.808614
Epoch:5 Loss:0.013244, TrainAcc:0.810020
Val_Loss:0.011465, Val_Acc:0.839056
Training iter 12, lr 5e-05
Epoch:1 Loss:0.013202, TrainAcc:0.810789
Epoch:2 Loss:0.013122, TrainAcc:0.811480
Epoch:3 Loss:0.013023, TrainAcc:0.813552
Epoch:4 Loss:0.012917, TrainAcc:0.814728
Epoch:5 Loss:0.012838, TrainAcc:0.815879
Val_Loss:0.011151, Val_Acc:0.842605
Training iter 13, lr 5e-05
Epoch:1 Loss:0.012762, TrainAcc:0.816750
Epoch:2 Loss:0.012688, TrainAcc:0.818161
Epoch:3 Loss:0.012605, TrainAcc:0.819000
Epoch:4 Loss:0.012478, TrainAcc:0.821052
Epoch:5 Loss:0.012444, TrainAcc:0.821703
Val_Loss:0.010967, Val_Acc:0.846133

Sample on yelp_validation_medium
INPUT: <CLS>it.wasn't even <MASK>usy, and we re<MASK>eatedly<MASK>had to tr<MASK>ck dow4 o<MASK>r ser<MASK>er to<MASK>ask for dri<MASK>ks,<MASK>foo&, a<MASK>d the <MASK>he<MASK>k.<SEP>
PREDICTION: <CLS>it wasn't even busy, and we repeatedly had to trick down our server to ask for drinks, food, and the check.<SEP>

# lr 5e-05 (result is yelp4_cp00 ~ 04.pt because i did big dumb)
Training iter 0, lr 5e-05
Epoch:1 Loss:0.012367, TrainAcc:0.822695
Epoch:2 Loss:0.012323, TrainAcc:0.823565
Epoch:3 Loss:0.012185, TrainAcc:0.825212
Epoch:4 Loss:0.012105, TrainAcc:0.826323
Epoch:5 Loss:0.012088, TrainAcc:0.826792
Val_Loss:0.010655, Val_Acc:0.850986
Training iter 1, lr 5e-05
Epoch:1 Loss:0.012012, TrainAcc:0.827648
Epoch:2 Loss:0.011938, TrainAcc:0.828727
Epoch:3 Loss:0.011851, TrainAcc:0.829957
Epoch:4 Loss:0.011768, TrainAcc:0.831224
Epoch:5 Loss:0.011722, TrainAcc:0.831796
Val_Loss:0.010400, Val_Acc:0.855180
Training iter 2, lr 5e-05
Epoch:1 Loss:0.011691, TrainAcc:0.832328
Epoch:2 Loss:0.011600, TrainAcc:0.833696
Epoch:3 Loss:0.011540, TrainAcc:0.834540
Epoch:4 Loss:0.011512, TrainAcc:0.835018
Epoch:5 Loss:0.011398, TrainAcc:0.836604
Val_Loss:0.010088, Val_Acc:0.858360
Training iter 3, lr 5e-05
Epoch:1 Loss:0.011351, TrainAcc:0.837227
Epoch:2 Loss:0.011295, TrainAcc:0.838196
Epoch:3 Loss:0.011289, TrainAcc:0.838301
Epoch:4 Loss:0.011179, TrainAcc:0.840252
Epoch:5 Loss:0.011158, TrainAcc:0.840412
Val_Loss:0.009789, Val_Acc:0.861725
Training iter 4, lr 5e-05
Epoch:1 Loss:0.011117, TrainAcc:0.840763
Epoch:2 Loss:0.011034, TrainAcc:0.841600
Epoch:3 Loss:0.010952, TrainAcc:0.842658
Epoch:4 Loss:0.010934, TrainAcc:0.843072
Epoch:5 Loss:0.010889, TrainAcc:0.843770
Val_Loss:0.009594, Val_Acc:0.865128

# Validation on validation_medium (yelp4_cp04.pt)
Val_Loss:0.009258, Val_Acc:0.870419
# Validation on validation_large (yelp4_cp04.pt)
Val_Loss:0.009463, Val_Acc:0.867619

# lr 5e-05 (result is yelp5_cp00.pt / for some reason it turned off)
Training iter 0, lr 5e-05
Epoch:1 Loss:0.010856, TrainAcc:0.844379
Epoch:2 Loss:0.010766, TrainAcc:0.846033
Epoch:3 Loss:0.010753, TrainAcc:0.845846
Epoch:4 Loss:0.010698, TrainAcc:0.846702
Epoch:5 Loss:0.010627, TrainAcc:0.847842
Val_Loss:0.009644, Val_Acc:0.865928

# lr 5e-05 (result is yelp6_cp--.pt)
Training iter 0, lr 5e-05
Epoch:1 Loss:0.010620, TrainAcc:0.847819
Epoch:2 Loss:0.010555, TrainAcc:0.848450
Epoch:3 Loss:0.010526, TrainAcc:0.848877
Epoch:4 Loss:0.010473, TrainAcc:0.849993
Epoch:5 Loss:0.010422, TrainAcc:0.850439
Val_Loss:0.009262, Val_Acc:0.872282
Training iter 1, lr 5e-05
Epoch:1 Loss:0.010382, TrainAcc:0.851310
Epoch:2 Loss:0.010351, TrainAcc:0.851659
Epoch:3 Loss:0.010253, TrainAcc:0.853066
Epoch:4 Loss:0.010247, TrainAcc:0.853382
Epoch:5 Loss:0.010176, TrainAcc:0.854008
Val_Loss:0.009088, Val_Acc:0.873781
Training iter 2, lr 5e-05
Epoch:1 Loss:0.010156, TrainAcc:0.854626
Epoch:2 Loss:0.010088, TrainAcc:0.855376
Epoch:3 Loss:0.010085, TrainAcc:0.855562
Epoch:4 Loss:0.010012, TrainAcc:0.856495
Epoch:5 Loss:0.009988, TrainAcc:0.857044
Val_Loss:0.008940, Val_Acc:0.875705
Training iter 3, lr 5e-05
Epoch:1 Loss:0.009965, TrainAcc:0.857260
Epoch:2 Loss:0.009891, TrainAcc:0.858084
Epoch:3 Loss:0.009911, TrainAcc:0.857904
Epoch:4 Loss:0.009831, TrainAcc:0.858825
Epoch:5 Loss:0.009824, TrainAcc:0.859114
Val_Loss:0.008844, Val_Acc:0.877855
Training iter 4, lr 5e-05
Epoch:1 Loss:0.009757, TrainAcc:0.859997
Epoch:2 Loss:0.009723, TrainAcc:0.860318
Epoch:3 Loss:0.009731, TrainAcc:0.860451
Epoch:4 Loss:0.009711, TrainAcc:0.860788
Epoch:5 Loss:0.009620, TrainAcc:0.861767
Val_Loss:0.008687, Val_Acc:0.879414
Training iter 5, lr 5e-05
Epoch:1 Loss:0.009582, TrainAcc:0.862522
Epoch:2 Loss:0.009563, TrainAcc:0.862873
Epoch:3 Loss:0.009528, TrainAcc:0.863163
Epoch:4 Loss:0.009526, TrainAcc:0.863238
Epoch:5 Loss:0.009475, TrainAcc:0.863666
Val_Loss:0.008502, Val_Acc:0.881892
Training iter 6, lr 5e-05
Epoch:1 Loss:0.009423, TrainAcc:0.864681
Epoch:2 Loss:0.009409, TrainAcc:0.865307
Epoch:3 Loss:0.009388, TrainAcc:0.865350
Epoch:4 Loss:0.009306, TrainAcc:0.866271
Epoch:5 Loss:0.009302, TrainAcc:0.866687
Val_Loss:0.008359, Val_Acc:0.883596
Training iter 7, lr 5e-05
Epoch:1 Loss:0.009292, TrainAcc:0.866526
Epoch:2 Loss:0.009258, TrainAcc:0.867134
Epoch:3 Loss:0.009213, TrainAcc:0.867803
Epoch:4 Loss:0.009210, TrainAcc:0.867649
Epoch:5 Loss:0.009149, TrainAcc:0.868908
Val_Loss:0.008269, Val_Acc:0.885086
Training iter 8, lr 5e-05
Epoch:1 Loss:0.009146, TrainAcc:0.868781
Epoch:2 Loss:0.009065, TrainAcc:0.869704
Epoch:3 Loss:0.009071, TrainAcc:0.869676
Epoch:4 Loss:0.009044, TrainAcc:0.870221
Epoch:5 Loss:0.008982, TrainAcc:0.870766
Val_Loss:0.008389, Val_Acc:0.883924
Training iter 9, lr 5e-05
Epoch:1 Loss:0.009008, TrainAcc:0.870461
Epoch:2 Loss:0.008944, TrainAcc:0.871588
Epoch:3 Loss:0.008957, TrainAcc:0.871560
Epoch:4 Loss:0.008898, TrainAcc:0.872226
Epoch:5 Loss:0.008886, TrainAcc:0.872464
Val_Loss:0.008013, Val_Acc:0.888270
Training iter 10, lr 5e-05
Epoch:1 Loss:0.008857, TrainAcc:0.872966
Epoch:2 Loss:0.008835, TrainAcc:0.873398
Epoch:3 Loss:0.008786, TrainAcc:0.873644
Epoch:4 Loss:0.008767, TrainAcc:0.874497
Epoch:5 Loss:0.008767, TrainAcc:0.874356
Val_Loss:0.007905, Val_Acc:0.890043
Training iter 11, lr 5e-05
Epoch:1 Loss:0.008720, TrainAcc:0.874597
Epoch:2 Loss:0.008722, TrainAcc:0.874533
Epoch:3 Loss:0.008656, TrainAcc:0.875843
Epoch:4 Loss:0.008682, TrainAcc:0.875284
Epoch:5 Loss:0.008653, TrainAcc:0.875799
Val_Loss:0.007908, Val_Acc:0.890158
Training iter 12, lr 5e-05
Epoch:1 Loss:0.008592, TrainAcc:0.876487
Epoch:2 Loss:0.008589, TrainAcc:0.876796
Epoch:3 Loss:0.008584, TrainAcc:0.876687
Epoch:4 Loss:0.008528, TrainAcc:0.877553
Epoch:5 Loss:0.008479, TrainAcc:0.878343
Val_Loss:0.007795, Val_Acc:0.890280


# CBERT_SST
# replace random
# seeds = [31, 41, 59, 2, 65, 35, 897, 93, 2384, 626]
# K = 0 ... 10
# validation, test respectively
0
0.7339448928833008 0.7144495248794556 0.732798159122467 0.7052751779556274 0.7236238121986389 0.7052751779556274 0.7167431116104126 0.7293577790260315 0.71100914478302 0.7121559381484985 AVG:  0.718463271856308
1
0.7190366387367249 0.7213302254676819 0.6961008906364441 0.7167431116104126 0.7144495248794556 0.71100914478302 0.7018348574638367 0.7041283845901489 0.7178899049758911 0.7006880640983582 AVG:  0.7103210747241974
2
0.6834862232208252 0.7224770188331604 0.7190366387367249 0.7144495248794556 0.7075687646865845 0.7052751779556274 0.7213302254676819 0.6961008906364441 0.7178899049758911 0.713302731513977 AVG:  0.7100917100906372
3
0.7155963182449341 0.7247706055641174 0.6972476840019226 0.7052751779556274 0.7075687646865845 0.6938073039054871 0.7167431116104126 0.7041283845901489 0.708715558052063 0.7213302254676819 AVG:  0.709518313407898
4
0.7201834321022034 0.6949540972709656 0.6926605105400085 0.6949540972709656 0.7052751779556274 0.71100914478302 0.6995412707328796 0.6903669238090515 0.689220130443573 0.6983944773674011 AVG:  0.6996559262275696
5
0.689220130443573 0.6788990497589111 0.6903669238090515 0.6949540972709656 0.706421971321106 0.7006880640983582 0.689220130443573 0.7075687646865845 0.7018348574638367 0.6743118762969971 AVG:  0.6933485865592957
6
0.6880733370780945 0.708715558052063 0.6811926364898682 0.6949540972709656 0.7178899049758911 0.6926605105400085 0.69151371717453 0.7098623514175415 0.6616972088813782 0.6903669238090515 AVG:  0.6936926245689392
7
0.6972476840019226 0.6823394298553467 0.6834862232208252 0.7075687646865845 0.6857798099517822 0.6961008906364441 0.6731650829315186 0.6834862232208252 0.6605504155158997 0.6754586696624756 AVG:  0.6845183193683624
8
0.6731650829315186 0.6869266033172607 0.6788990497589111 0.6857798099517822 0.6995412707328796 0.6525229215621948 0.6880733370780945 0.6788990497589111 0.6846330165863037 0.6823394298553467 AVG:  0.6810779571533203
9
0.6628440022468567 0.6628440022468567 0.6846330165863037 0.6903669238090515 0.689220130443573 0.6869266033172607 0.6811926364898682 0.6857798099517822 0.6559633016586304 0.6777522563934326 AVG:  0.6777522683143615
10
0.6536697149276733 0.6754586696624756 0.6720183491706848 0.6846330165863037 0.6857798099517822 0.6743118762969971 0.6869266033172607 0.6834862232208252 0.6731650829315186 0.6720183491706848 AVG:  0.6761467695236206
================================================================================
0
0.6994505524635315 0.6939560770988464 0.6895604729652405 0.7170330286026001 0.6994505524635315 0.7060440182685852 0.696703314781189 0.7170330286026001 0.7005494832992554 0.7021978497505188 AVG:  0.7021978378295899
1
0.6956044435501099 0.6950549483299255 0.69010990858078 0.6934066414833069 0.6961538791656494 0.7032967209815979 0.7032967209815979 0.6961538791656494 0.682417631149292 0.6939560770988464 AVG:  0.6949450850486756
2
0.6906593441963196 0.6884615421295166 0.684615433216095 0.6956044435501099 0.69010990858078 0.692307710647583 0.6862637400627136 0.6961538791656494 0.6934066414833069 0.6939560770988464 AVG:  0.6911538720130921
3
0.686813235282898 0.682417631149292 0.6835165023803711 0.692307710647583 0.6895604729652405 0.6906593441963196 0.687912106513977 0.6791208982467651 0.6912088394165039 0.6890110373497009 AVG:  0.6872527778148652
4
0.6989011168479919 0.6796703338623047 0.6747252941131592 0.6670330166816711 0.6939560770988464 0.7016483545303345 0.6961538791656494 0.6681318879127502 0.6752747297286987 0.6818681359291077 AVG:  0.6837362825870514
5
0.6719780564308167 0.6703296899795532 0.680219829082489 0.6648352146148682 0.6873626708984375 0.6851648688316345 0.6692308187484741 0.6752747297286987 0.6609890460968018 0.6752747297286987 AVG:  0.6740659654140473
6
0.6714286208152771 0.6686813235282898 0.6692308187484741 0.6620879173278809 0.6703296899795532 0.6692308187484741 0.6686813235282898 0.6840659379959106 0.6840659379959106 0.6697802543640137 AVG:  0.6717582643032074
7
0.6818681359291077 0.6670330166816711 0.6719780564308167 0.653296709060669 0.6543956398963928 0.6681318879127502 0.6494506001472473 0.678022027015686 0.682417631149292 0.687912106513977 AVG:  0.669450581073761
8
0.6521978378295898 0.6543956398963928 0.6615384817123413 0.6609890460968018 0.6620879173278809 0.6604396104812622 0.6582418084144592 0.6697802543640137 0.6774725317955017 0.6527472734451294 AVG:  0.6609890401363373
9
0.6648352146148682 0.6527472734451294 0.6648352146148682 0.648901104927063 0.6571428775787354 0.6741758584976196 0.6434066295623779 0.6615384817123413 0.6708791255950928 0.6642857193946838 AVG:  0.6602747499942779
10
0.6747252941131592 0.6549450755119324 0.6483516693115234 0.6664835214614868 0.6549450755119324 0.6609890460968018 0.6571428775787354 0.6626374125480652 0.6719780564308167 0.6494506001472473 AVG:  0.66016486287117

# BERT_SST
# Same settings
0
0.9094036697247706 0.9094036697247706 0.9094036697247706 0.9094036697247706 0.9094036697247706 0.9094036697247706 0.9094036697247706 0.9094036697247706 0.9094036697247706 0.9094036697247706 AVG:  0.9094036697247707
1
0.8887614678899083 0.8784403669724771 0.9013761467889908 0.8772935779816514 0.893348623853211 0.8979357798165137 0.8979357798165137 0.8967889908256881 0.8887614678899083 0.8876146788990825 AVG:  0.8908256880733945
2
0.8692660550458715 0.8715596330275229 0.8887614678899083 0.8772935779816514 0.8635321100917431 0.8864678899082569 0.8669724770642202 0.8807339449541285 0.8658256880733946 0.8795871559633027 AVG:  0.8750000000000002
3
0.8337155963302753 0.8405963302752294 0.856651376146789 0.8543577981651376 0.8325688073394495 0.8348623853211009 0.8520642201834863 0.8486238532110092 0.8428899082568807 0.8405963302752294 AVG:  0.8436926605504587
4
0.8153669724770642 0.8153669724770642 0.8188073394495413 0.8279816513761468 0.8222477064220184 0.8073394495412844 0.8325688073394495 0.8268348623853211 0.8256880733944955 0.8279816513761468 AVG:  0.8220183486238533
5
0.7947247706422018 0.7958715596330275 0.7993119266055045 0.7878440366972477 0.7763761467889908 0.7947247706422018 0.7889908256880734 0.786697247706422 0.7912844036697247 0.7889908256880734 AVG:  0.7904816513761468
6
0.7821100917431193 0.7706422018348624 0.7763761467889908 0.7752293577981652 0.7672018348623854 0.7557339449541285 0.7694954128440367 0.7660550458715596 0.768348623853211 0.7591743119266054 AVG:  0.7690366972477064
7
0.7408256880733946 0.7339449541284404 0.7488532110091743 0.7522935779816514 0.7282110091743119 0.7419724770642202 0.7327981651376146 0.7477064220183486 0.7408256880733946 0.75 AVG:  0.7417431192660551
8
0.7155963302752294 0.7121559633027523 0.7224770642201835 0.7327981651376146 0.7247706422018348 0.713302752293578 0.7075688073394495 0.713302752293578 0.6938073394495413 0.7362385321100917 AVG:  0.7172018348623853
9
0.6926605504587156 0.6880733944954128 0.7052752293577982 0.7041284403669725 0.6892201834862385 0.6972477064220184 0.7029816513761468 0.6961009174311926 0.6938073394495413 0.6995412844036697 AVG:  0.6969036697247706
10
0.6731651376146789 0.6731651376146789 0.6903669724770642 0.6857798165137615 0.6754587155963303 0.6731651376146789 0.6743119266055045 0.6708715596330275 0.6651376146788991 0.676605504587156 AVG:  0.675802752293578
================================================================================
0
0.9225700164744646 0.9225700164744646 0.9225700164744646 0.9225700164744646 0.9225700164744646 0.9225700164744646 0.9225700164744646 0.9225700164744646 0.9225700164744646 0.9225700164744646 AVG:  0.9225700164744646
1
0.9066447007138935 0.9033498077979132 0.8945634266886326 0.9038989566172433 0.9038989566172433 0.9017023613399231 0.9017023613399231 0.9044481054365733 0.9044481054365733 0.9022515101592532 AVG:  0.9026908292147173
2
0.8835804503020318 0.8764415156507414 0.8764415156507414 0.8846787479406919 0.885777045579352 0.885777045579352 0.8802855573860516 0.8747940691927513 0.8819330038440417 0.8824821526633718 AVG:  0.8812191103789127
3
0.8489840746842394 0.8544755628775398 0.8588687534321802 0.844041735310269 0.8539264140582098 0.85722130697419 0.8533772652388797 0.85612300933553 0.8616144975288303 0.8511806699615596 AVG:  0.8539813289401428
4
0.8303130148270181 0.8220757825370676 0.8319604612850082 0.8160351455244371 0.8270181219110379 0.8330587589236683 0.8237232289950577 0.828665568369028 0.8352553542009885 0.828116419549698 AVG:  0.827622185612301
5
0.7995606809445359 0.8023064250411862 0.8083470620538166 0.7907742998352554 0.8017572762218561 0.8017572762218561 0.8023064250411862 0.8061504667764964 0.8039538714991763 0.7946183415705657 AVG:  0.801153212520593
6
0.7726523887973641 0.7627677100494233 0.7781438769906645 0.7688083470620538 0.771554091158704 0.7688083470620538 0.7841845140032949 0.7721032399780341 0.7726523887973641 0.7753981328940143 AVG:  0.7727073036792971
7
0.7528830313014827 0.7473915431081823 0.7391543108182317 0.741900054914882 0.7556287753981329 0.7402526084568918 0.741900054914882 0.7484898407468424 0.7523338824821527 0.7364085667215815 AVG:  0.7456342668863261
8
0.7303679297089511 0.7177375068643602 0.7221306974190006 0.7160900604063701 0.7193849533223503 0.7084019769357496 0.7210323997803405 0.7193849533223503 0.7287204832509611 0.7204832509610104 AVG:  0.7203734211971444
9
0.7029104887424492 0.6935749588138386 0.7040087863811093 0.6968698517298187 0.6930258099945085 0.6979681493684788 0.7040087863811093 0.6952224052718287 0.6968698517298187 0.7067545304777595 AVG:  0.6991213618890719
10
0.6760021965952773 0.6655683690280065 0.6743547501372872 0.6644700713893466 0.6672158154859967 0.6798462383305875 0.6771004942339374 0.6814936847885777 0.6820428336079077 0.6776496430532675 AVG:  0.6745744096650192



# CBERT_SST
# gen_adv
# seeds = [31, 41, 59, 2, 65, 35, 897, 93, 2384, 626]
# K = 0 ... 10
# validation, test respectively
0
0.7396788597106934 0.7396788597106934 0.7396788597106934 0.7396788597106934 0.7396788597106934 0.7396788597106934 0.7396788597106934 0.7396788597106934 0.7396788597106934 0.7396788597106934 AVG:  0.7396788597106934
1
0.73050457239151 0.73050457239151 0.728210985660553 0.7155963182449341 0.7201834321022034 0.7373852729797363 0.725917398929596 0.7270641922950745 0.732798159122467 0.7190366387367249 AVG:  0.7267201542854309
2
0.728210985660553 0.7385320663452148 0.728210985660553 0.7247706055641174 0.7339448928833008 0.7224770188331604 0.7293577790260315 0.728210985660553 0.7316513657569885 0.7293577790260315 AVG:  0.7294724464416504
3
0.7213302254676819 0.7098623514175415 0.7373852729797363 0.7201834321022034 0.7213302254676819 0.6995412707328796 0.71100914478302 0.7201834321022034 0.7029816508293152 0.7224770188331604 AVG:  0.7166284024715424
4
0.7373852729797363 0.7075687646865845 0.708715558052063 0.7041283845901489 0.728210985660553 0.71100914478302 0.725917398929596 0.7144495248794556 0.7201834321022034 0.7247706055641174 AVG:  0.7182339072227478
5
0.7041283845901489 0.7408256530761719 0.7201834321022034 0.7224770188331604 0.6995412707328796 0.725917398929596 0.689220130443573 0.7121559381484985 0.7006880640983582 0.706421971321106 AVG:  0.7121559262275696
6
0.7018348574638367 0.7167431116104126 0.7052751779556274 0.6961008906364441 0.6926605105400085 0.7144495248794556 0.6995412707328796 0.728210985660553 0.7270641922950745 0.6983944773674011 AVG:  0.7080274999141694
7
0.713302731513977 0.713302731513977 0.7006880640983582 0.7201834321022034 0.7006880640983582 0.7041283845901489 0.6995412707328796 0.7224770188331604 0.706421971321106 0.732798159122467 AVG:  0.7113531827926636
8
0.7224770188331604 0.6961008906364441 0.6926605105400085 0.6938073039054871 0.6846330165863037 0.6926605105400085 0.6834862232208252 0.7098623514175415 0.6766054630279541 0.689220130443573 AVG:  0.6941513419151306
9
0.69151371717453 0.6938073039054871 0.689220130443573 0.706421971321106 0.706421971321106 0.6972476840019226 0.6880733370780945 0.6949540972709656 0.689220130443573 0.6869266033172607 AVG:  0.6943806946277619
10
0.6983944773674011 0.69151371717453 0.7006880640983582 0.7075687646865845 0.6743118762969971 0.6777522563934326 0.6903669238090515 0.6938073039054871 0.6903669238090515 0.6823394298553467 AVG:  0.690710973739624
================================================================================
0
0.7318681478500366 0.7318681478500366 0.7318681478500366 0.7318681478500366 0.7318681478500366 0.7318681478500366 0.7318681478500366 0.7318681478500366 0.7318681478500366 0.7318681478500366 AVG:  0.7318681478500366
1
0.7263736724853516 0.735714316368103 0.7351648807525635 0.7368132472038269 0.7285714745521545 0.735714316368103 0.7247253060340881 0.7291209101676941 0.730219841003418 0.7252747416496277 AVG:  0.7307692706584931
2
0.7104396224021912 0.7204832434654236 0.7248764634132385 0.7089511156082153 0.7166392207145691 0.7252747416496277 0.7177374958992004 0.7192308306694031 0.7177374958992004 0.7197802662849426 AVG:  0.7181150496006012
3
0.7144426107406616 0.7199341058731079 0.7082418203353882 0.713893473148346 0.7120879292488098 0.7082418203353882 0.7073037028312683 0.7133443355560303 0.7127951979637146 0.7149917483329773 AVG:  0.7125276744365692
4
0.7087912559509277 0.7111477255821228 0.7082418203353882 0.7076923251152039 0.7007138729095459 0.7043956518173218 0.7005494832992554 0.7133443355560303 0.7281713485717773 0.7199341058731079 AVG:  0.7102981925010681
5
0.692307710647583 0.7160900831222534 0.7005494832992554 0.7111477255821228 0.7027472853660583 0.7038462162017822 0.7149917483329773 0.7126374244689941 0.7040088176727295 0.7032967209815979 AVG:  0.7061623215675354
6
0.6891818046569824 0.7104396224021912 0.6847885847091675 0.694505512714386 0.7060440182685852 0.6928571462631226 0.6908292174339294 0.7005494832992554 0.7020341157913208 0.6913783550262451 AVG:  0.6962607860565185
7
0.6858868598937988 0.6758242249488831 0.6853377223014832 0.6906593441963196 0.6961538791656494 0.6880834698677063 0.6946732401847839 0.6935749650001526 0.6956044435501099 0.6785714626312256 AVG:  0.6884369611740112
8
0.6908292174339294 0.678022027015686 0.6736264228820801 0.6860912442207336 0.6968698501586914 0.6895604729652405 0.6796703338623047 0.686985194683075 0.6862637400627136 0.6730769276618958 AVG:  0.684099543094635
9
0.6873626708984375 0.6792970895767212 0.6725274920463562 0.6741758584976196 0.678022027015686 0.7062053680419922 0.6738055944442749 0.6913783550262451 0.6738055944442749 0.6771004796028137 AVG:  0.6813680529594421
10
0.674903929233551 0.6897309422492981 0.6648352146148682 0.6761957406997681 0.6864360570907593 0.6668499112129211 0.680944561958313 0.6763736605644226 0.6844419836997986 0.6659340858459473 AVG:  0.6766646087169648

# BERT_SST
# Same settings
0
0.9105504587155964 0.9105504587155964 0.9105504587155964 0.9105504587155964 0.9105504587155964 0.9105504587155964 0.9105504587155964 0.9105504587155964 0.9105504587155964 0.9105504587155964 AVG:  0.9105504587155961
1
0.8922018348623854 0.8887614678899083 0.8864678899082569 0.8830275229357798 0.9025229357798165 0.8807339449541285 0.8910550458715596 0.8944954128440367 0.8956422018348624 0.8887614678899083 AVG:  0.8903669724770642
2
0.8589449541284404 0.8646788990825688 0.8830275229357798 0.8658256880733946 0.856651376146789 0.8772935779816514 0.8658256880733946 0.8635321100917431 0.875 0.8635321100917431 AVG:  0.8674311926605505
3
0.8360091743119266 0.8428899082568807 0.8612385321100917 0.8474770642201835 0.8463302752293578 0.8589449541284404 0.8360091743119266 0.8589449541284404 0.8543577981651376 0.8360091743119266 AVG:  0.8478211009174312
4
0.8268348623853211 0.8302752293577982 0.8348623853211009 0.823394495412844 0.8256880733944955 0.8314220183486238 0.838302752293578 0.8428899082568807 0.8256880733944955 0.8211009174311926 AVG:  0.8300458715596329
5
0.8073394495412844 0.8027522935779816 0.8107798165137615 0.8119266055045872 0.823394495412844 0.8084862385321101 0.8084862385321101 0.8004587155963303 0.805045871559633 0.7924311926605505 AVG:  0.8071100917431192
6
0.7821100917431193 0.7821100917431193 0.7878440366972477 0.7786697247706422 0.7912844036697247 0.7775229357798165 0.7752293577981652 0.7660550458715596 0.8004587155963303 0.7752293577981652 AVG:  0.7816513761467889
7
0.7534403669724771 0.7534403669724771 0.7591743119266054 0.7557339449541285 0.7580275229357798 0.7488532110091743 0.7603211009174312 0.7614678899082569 0.7614678899082569 0.7614678899082569 AVG:  0.7573394495412844
8
0.7419724770642202 0.7454128440366973 0.7213302752293578 0.7385321100917431 0.7259174311926605 0.7293577981651376 0.7396788990825688 0.75 0.7270642201834863 0.731651376146789 AVG:  0.7350917431192661
9
0.7041284403669725 0.7293577981651376 0.7087155963302753 0.7110091743119266 0.7178899082568807 0.7155963302752294 0.7064220183486238 0.6926605504587156 0.7121559633027523 0.716743119266055 AVG:  0.7114678899082569
10
0.7041284403669725 0.6915137614678899 0.6811926605504587 0.6880733944954128 0.6972477064220184 0.7029816513761468 0.6995412844036697 0.7006880733944955 0.6892201834862385 0.7052752293577982 AVG:  0.6959862385321101
================================================================================
0
0.9236683141131247 0.9236683141131247 0.9236683141131247 0.9236683141131247 0.9236683141131247 0.9236683141131247 0.9236683141131247 0.9236683141131247 0.9236683141131247 0.9236683141131247 AVG:  0.9236683141131247
1
0.9093904448105437 0.900054914881933 0.9017023613399231 0.9093904448105437 0.8967600219659527 0.9121361889071938 0.9060955518945635 0.9060955518945635 0.9033498077979132 0.9011532125205931 AVG:  0.9046128500823724
2
0.885777045579352 0.8885227896760022 0.885227896760022 0.8830313014827018 0.8824821526633718 0.8879736408566722 0.8879736408566722 0.8786381109280615 0.885777045579352 0.886326194398682 AVG:  0.8851729818780891
3
0.844041735310269 0.8506315211422295 0.8594179022515102 0.8599670510708401 0.8687534321801208 0.8577704557935201 0.8742449203734212 0.85722130697419 0.8517298187808896 0.8506315211422295 AVG:  0.8574409665019219
4
0.8209774848984075 0.8434925864909391 0.828665568369028 0.829214717188358 0.8363536518396485 0.8341570565623284 0.8401976935749588 0.8325096101043383 0.8390993959362988 0.8314113124656782 AVG:  0.8336079077429984
5
0.8066996155958265 0.8094453596924767 0.8160351455244371 0.8182317408017573 0.8045030203185063 0.8176825919824272 0.8171334431630972 0.8088962108731467 0.8017572762218561 0.8116419549697969 AVG:  0.8112026359143328
6
0.7699066447007139 0.7935200439319056 0.7907742998352554 0.7924217462932455 0.786381109280615 0.7907742998352554 0.7748489840746843 0.7896760021965953 0.7803404722679846 0.7737506864360242 AVG:  0.7842394288852279
7
0.7583745194947831 0.7616694124107634 0.7506864360241625 0.7550796265788029 0.7644151565074135 0.7616694124107634 0.7699066447007139 0.7655134541460736 0.7594728171334432 0.7732015376166941 AVG:  0.7619989017023615
8
0.7380560131795717 0.727622185612301 0.7347611202635914 0.729269632070291 0.7336628226249313 0.7413509060955519 0.7353102690829215 0.7353102690829215 0.7325645249862712 0.7397034596375618 AVG:  0.7347611202635914
9
0.7160900604063701 0.7210323997803405 0.7232289950576606 0.7204832509610104 0.7210323997803405 0.7380560131795717 0.7078528281164196 0.7199341021416804 0.7188358045030203 0.7095002745744097 AVG:  0.7196046128500824
10
0.7056562328390994 0.6924766611751785 0.6897309170785283 0.6836902800658978 0.6968698517298187 0.7040087863811093 0.6952224052718287 0.6908292147171884 0.6952224052718287 0.6946732564524987 AVG:  0.6948380010982976