{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefb4de-c0ed-4064-9290-e54f86d77252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "SST = load_dataset(\"SetFit/sst2\")\n",
    "SST = SST['validation'] # Select dataset to preprocess ['train'], ['validation'], ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5acb66-0d66-4f8f-9263-5c9c68dbbba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_char = ['<PAD>', '<CLS>', '<SEP>', '<MASK>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', \\\n",
    "              '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
    "char_to_id = {c:i for i,c in enumerate(id_to_char)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ba261-30e5-4d5a-8019-7b38583548bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = ['-lrb-', '-rrb-', 'ã§', 'ã¯', 'ã£', 'ã¨', 'ã»', 'ã¶', 'ã±', 'ã¢', 'ã-', 'ã¡', 'ã¦', 'ã³', 'ã©', 'ã¼', 'ü', 'û', 'ñ', 'ó', 'ô', 'ö', 'í', 'ï', 'mollã', 'jirã', 'ã', '\\xad', '¼', '³', '¡', '¦', '\\xa0', '¢', 'ç', '´', 'à', 'á', 'â', 'é', 'è', 'æ' ]\n",
    "tok = ['('    , ')'    , 'c' , 'i' , 'a' , 'e' , 'u' , 'o' , 'n' , 'a' , 'i' , 'a' , 'ae', 'o' , 'e' , 'u' , 'u', 'u', 'n', 'o', 'o', 'o', 'i', 'i', 'molla', 'jiri', 'a', ''    , '' , '' , '' , '' , ''    , 'c', 'c', '' , 'a', 'a', 'a', 'e', 'e', 'ae']\n",
    "assert(len(rep)==len(tok))\n",
    "\n",
    "def refine_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    # Cut out \\n|\n",
    "    if sent[-1]=='\\n': sent=sent[:-1]\n",
    "    # Repleace LRB, RRB to (, ) respectively\n",
    "    for f, t in zip(rep, tok):\n",
    "        sent = sent.replace(f, t)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26f5e8-d3e7-453f-bfe5-2c8241ab4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [] # Ids for each character\n",
    "encoder_mask = [] # Mask for <PAD> tokens\n",
    "word_idx = [] # Indexes of ends of each word\n",
    "num_words = [] # Number of words\n",
    "num_chars = [] # Number of characters\n",
    "labels = []\n",
    "len_limit = 256\n",
    "for text, label in zip(SST['text'], SST['label']):\n",
    "    text = refine_sentence(text)\n",
    "    \n",
    "    d = [char_to_id['<CLS>']] # for input_ids\n",
    "    w = [-1] # for word_idx, -1 for <CLS>\n",
    "    n = 1 # for num_words\n",
    "    for j, c in enumerate(text):\n",
    "        d.append(char_to_id[c])\n",
    "        if c==' ':\n",
    "            w.append(0)\n",
    "            n+=1\n",
    "        else:\n",
    "            w.append(n)\n",
    "            \n",
    "    w.append(n)\n",
    "    len_d = len(d)-1 # Length except <CLS> and <SEP>\n",
    "    d.append(char_to_id['<SEP>'])\n",
    "    if len(d)<=len_limit: # Only add sentences with acceptable length\n",
    "        mask = [0]*len(d) + [1]*(len_limit-len(d)) # Create mask\n",
    "        d += [char_to_id['<PAD>']]*(len_limit-len(d)) # PAD current sentence\n",
    "        w.append(-1) # <SEP>\n",
    "        w += [-1]*(len_limit-len(w)) \n",
    "\n",
    "        # Append all to dataset\n",
    "        input_ids.append(d)\n",
    "        encoder_mask.append(mask)\n",
    "        word_idx.append(w)\n",
    "        num_words.append(n)\n",
    "        num_chars.append(len_d)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Reset variables to start a new sentence\n",
    "    d = [char_to_id['<CLS>']]\n",
    "    w = [-1]\n",
    "    n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ae77a-7079-4986-8a9e-5dc754c92b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of data lost due to length limit\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29240f-b8d4-4835-a5a2-a184c27102c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "import pandas as pd\n",
    "dataset = {\"input_ids\":input_ids, \"encoder_mask\":encoder_mask, \"label\":labels, \"word_idx\":word_idx, \"num_words\":num_words, \"num_chars\":num_chars}\n",
    "dataframe = pd.DataFrame(dataset).to_pickle(\"-your path-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed6eba-2e66-48e2-8a46-10b4f6f6fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "for i in range(10):\n",
    "    print(SST['text'][i])\n",
    "    for j in input_ids[i]:\n",
    "        if j!=1: print(id_to_char[j], end='')\n",
    "        if j==2: break\n",
    "    print('\\n',sum(encoder_mask[i]), labels[i], SST['label'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642cba00-b287-40eb-8f17-45aae0a439f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
