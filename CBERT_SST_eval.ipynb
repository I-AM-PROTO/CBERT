{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc871d-c9a3-471b-9442-13115e9aa615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a378c01-41d2-4157-9bd3-83930a19e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"SetFit/sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9724cf-3e4e-4730-a0fe-d22244ecfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datasets\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "id_to_char = ['<PAD>', '<CLS>', '<SEP>', '<MASK>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', \\\n",
    "              '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
    "char_to_id = {c:i for i,c in enumerate(id_to_char)}\n",
    "chars = [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', \\\n",
    "         '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
    "rep = ['-lrb-', '-rrb-', 'ã§', 'ã¯', 'ã£', 'ã¨', 'ã»', 'ã¶', 'ã±', 'ã¢', 'ã-', 'ã¡', 'ã¦', 'ã³', 'ã©', 'ã¼', 'ü', 'û', 'ñ', 'ó', 'ô', 'ö', 'í', 'ï', 'mollã', 'jirã', 'ã', '\\xad', '¼', '³', '¡', '¦', '\\xa0', '¢', 'ç', '´', 'à', 'á', 'â', 'é', 'è', 'æ' ]\n",
    "tok = ['('    , ')'    , 'c' , 'i' , 'a' , 'e' , 'u' , 'o' , 'n' , 'a' , 'i' , 'a' , 'ae', 'o' , 'e' , 'u' , 'u', 'u', 'n', 'o', 'o', 'o', 'i', 'i', 'molla', 'jiri', 'a', ''    , '' , '' , '' , '' , ''    , 'c', 'c', '' , 'a', 'a', 'a', 'e', 'e', 'ae']\n",
    "assert(len(rep)==len(tok))\n",
    "\n",
    "num_replace = 1\n",
    "num_mod = 1\n",
    "global_seed = 1\n",
    "\n",
    "def gen_adv(examples):\n",
    "    ret = []\n",
    "    np.random.seed(global_seed)\n",
    "    random.seed(global_seed)\n",
    "    for text in examples['text']:\n",
    "        text = refine_sentence(text)\n",
    "        text = list(text)\n",
    "        \n",
    "        rpl = random.random() \n",
    "        ins = random.random()\n",
    "        rmv = random.random()\n",
    "        rpl = rpl/(rpl+ins+rmv)\n",
    "        ins = ins/(rpl+ins+rmv)\n",
    "        rpl = math.floor(num_mod*rpl)\n",
    "        ins = math.floor(num_mod*ins)\n",
    "        rmv = num_mod - rpl - ins\n",
    "        \n",
    "        #remove\n",
    "        rnd = np.random.permutation(len(text))[:rmv]\n",
    "        rnd *= -1\n",
    "        rnd.sort()\n",
    "        rnd *= -1\n",
    "        for i in rnd:\n",
    "            del text[i]\n",
    "        \n",
    "        #replace\n",
    "        rnd = np.random.permutation(len(text))[:rpl]\n",
    "        for i in rnd:\n",
    "            text[i] = random.choice(chars)\n",
    "        \n",
    "        #insert\n",
    "        rnd = np.random.permutation(len(text))[:ins]\n",
    "        rnd *= -1\n",
    "        rnd.sort()\n",
    "        rnd *= -1\n",
    "        for i in rnd:\n",
    "            text.insert(i, random.choice(chars))\n",
    "            \n",
    "        ret.append(''.join(text))\n",
    "    return {'text':ret}\n",
    "\n",
    "# Deprecated\n",
    "def replace_random(examples):\n",
    "    ret = []\n",
    "    np.random.seed(global_seed)\n",
    "    random.seed(global_seed)\n",
    "    for text in examples['text']:\n",
    "        text = refine_sentence(text)\n",
    "        text = list(text)\n",
    "        rnd = np.random.permutation(len(text))\n",
    "        for i in range(min(num_replace,len(rnd))):\n",
    "            text[rnd[i]] = random.choice(chars)\n",
    "        ret.append(''.join(text))\n",
    "    return {'text':ret}\n",
    "\n",
    "def refine_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    # Cut out \\n|\n",
    "    if sent[-1]=='\\n': sent=sent[:-1]\n",
    "    # Repleace LRB, RRB to (, ) respectively\n",
    "    for f, t in zip(rep, tok):\n",
    "        sent = sent.replace(f, t)\n",
    "    return sent\n",
    "\n",
    "def refine_dataset(texts):\n",
    "    ret = []\n",
    "    for text in texts:\n",
    "        text = refine_sentence(text)\n",
    "        ret.append(text)\n",
    "    return ret\n",
    "\n",
    "def preprocess(dataset):\n",
    "    input_ids = [] # Ids for each character\n",
    "    encoder_mask = [] # Mask for <PAD> tokens\n",
    "    word_idx = [] # Indexes of ends of each word\n",
    "    num_words = [] # Number of words\n",
    "    num_chars = [] # Number of characters\n",
    "    labels = []\n",
    "    len_limit = 256\n",
    "    \n",
    "    for text, label in zip(dataset['text'], dataset['label']):\n",
    "        d = [char_to_id['<CLS>']] # for input_ids\n",
    "        w = [-1] # for word_idx, -1 for <CLS>\n",
    "        n = 1 # for num_words\n",
    "        for j, c in enumerate(text):\n",
    "            d.append(char_to_id[c])\n",
    "            if c==' ':\n",
    "                w.append(0)\n",
    "                n+=1\n",
    "            else:\n",
    "                w.append(n)\n",
    "\n",
    "        w.append(n)\n",
    "        len_d = len(d)-1 # Length except <CLS> and <SEP>\n",
    "        d.append(char_to_id['<SEP>'])\n",
    "        if len(d)<=len_limit: # Only add sentences with acceptable length\n",
    "            mask = [0]*len(d) + [1]*(len_limit-len(d)) # Create mask\n",
    "            d += [char_to_id['<PAD>']]*(len_limit-len(d)) # PAD current sentence\n",
    "            w.append(-1) # <SEP>\n",
    "            w += [-1]*(len_limit-len(w)) \n",
    "\n",
    "            # Append all to dataset\n",
    "            input_ids.append(d)\n",
    "            encoder_mask.append(mask)\n",
    "            word_idx.append(w)\n",
    "            num_words.append(n)\n",
    "            num_chars.append(len_d)\n",
    "            labels.append(label)\n",
    "\n",
    "        # Reset variables to start a new sentence\n",
    "        d = [char_to_id['<CLS>']]\n",
    "        w = [-1]\n",
    "        n = 1\n",
    "        \n",
    "    input_ids = [x for x in torch.tensor(input_ids)]\n",
    "    encoder_mask = [x for x in torch.tensor(encoder_mask)]\n",
    "    label = [x for x in torch.tensor(labels)]\n",
    "    dataframe = list(zip(input_ids, encoder_mask, label))\n",
    "    return DataLoader(dataframe, batch_size=48, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd3f23-faeb-4d0b-9422-eab37be10a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import copy\n",
    "\n",
    "# Vocab\n",
    "id_to_char = ['<PAD>', '<CLS>', '<SEP>', '<MASK>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', \\\n",
    "              '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
    "char_to_id = {c:i for i,c in enumerate(id_to_char)}\n",
    "num_spechar = 4\n",
    "vocab_size = len(id_to_char)-num_spechar\n",
    "data_max_len = 256\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_idx, max_seq_len, drop_prob):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.char_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_size)\n",
    "        self.LayerNorm = nn.LayerNorm(embed_size, eps=1e-7)\n",
    "        self.dropout = nn.Dropout(drop_prob)  # 0.1\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        position_ids = torch.arange(self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        words_embeddings = self.char_embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings  # (batchSize, sequenceLength, hidden_size)\n",
    "\n",
    "class CBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dim_feedforward, num_heads, num_layers, pad_idx):\n",
    "        super(CBERT, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, embed_size, pad_idx, data_max_len, 0.1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.prediction_layer = nn.Linear(embed_size, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        embedded_x = self.embedding_layer(x)\n",
    "        encoded_x = self.transformer_encoder(embedded_x, src_key_padding_mask=mask)\n",
    "        #return torch.flatten(encoded_x, start_dim=1)\n",
    "        return encoded_x[:,0,:]\n",
    "        #return torch.mean(encoded_x, 1, False)\n",
    "    \n",
    "class CBERT_SA(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dim_feedforward, num_heads, num_layers, pad_idx, CBERT_PATH, num_class=2):\n",
    "        super(CBERT_SA, self).__init__()\n",
    "        self.CBERT = CBERT(vocab_size, embed_size, dim_feedforward, num_heads, num_layers, pad_idx)\n",
    "        self.prediction_layer = nn.Linear(embed_size, num_class)\n",
    "        #self.prediction_layer = nn.Sequential(nn.Linear(embed_size*256, num_class))\n",
    "        #self.prediction_layer = nn.Sequential(nn.Linear(embed_size, num_class))\n",
    "        #self.prediction_layer = nn.Sequential(nn.Linear(embed_size, 30), nn.ReLU(), nn.Linear(30, num_class))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # init\n",
    "        checkpoint = torch.load(CBERT_PATH)\n",
    "        self.CBERT.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        self.prediction_layer.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        #print(x.shape)\n",
    "        encoded_x = self.CBERT(x, mask)\n",
    "        #print(encoded_x.shape)\n",
    "        pred = self.prediction_layer(encoded_x)\n",
    "        #print(pred.shape)\n",
    "        ret = self.softmax(pred)\n",
    "        #print(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da6e34-00d9-4b34-9cd2-811faa3715ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBERT_SA(vocab_size=len(id_to_char),\n",
    "                 embed_size=768,\n",
    "                 dim_feedforward=2048,\n",
    "                 num_heads=12,\n",
    "                 num_layers=6,\n",
    "                 pad_idx=char_to_id['<PAD>'],\n",
    "                 CBERT_PATH = \"./models/yelp4_cp12.pt\")\n",
    "model = model.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd9b7c-1a24-46b3-8b08-26fa41cba4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = \"./models/SST_cp0.pt\"\n",
    "checkpoint = torch.load(LOAD_PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83287716-adf7-41cf-a027-12dfb5696c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_f, val_iter, device='cpu'):\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum = torch.tensor([0.0], device=device)\n",
    "        acc_sum = torch.tensor([0.0], device=device)\n",
    "        n = 0\n",
    "        \n",
    "        for x, mask, y in tqdm(val_iter):\n",
    "            x, mask, y = x.to(device=device), mask.to(device=device), y.to(device=device)\n",
    "            y_hat = model(x, mask)            \n",
    "            loss = loss_f(y_hat, y)\n",
    "        \n",
    "            loss_sum += loss.float()\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            acc_sum += torch.sum(pred==y)\n",
    "            n += x.shape[0]\n",
    "    \n",
    "    return (loss_sum/n, acc_sum/n)\n",
    "\n",
    "    \n",
    "def print_decoded_ids(ids):\n",
    "    for c in ids:\n",
    "        if c==0: # <PAD>, <SEP>\n",
    "            break\n",
    "        else:\n",
    "            print(id_to_char[c], end='')\n",
    "    print()    \n",
    "\n",
    "def show_sample(model, loss_f, data_iter, device='cpu'):\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        idx=0\n",
    "        for x, mask, y in data_iter:\n",
    "            x, mask, y = x.to(device=device), mask.to(device=device), y.to(device=device)\n",
    "            model.train()\n",
    "            y_hat = model(x, mask)            \n",
    "            loss = loss_f(y_hat, y)\n",
    "\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            for i,p,t in zip(x, pred, y):\n",
    "                print(\"INPUT:\", end=' ')\n",
    "                print_decoded_ids(i)\n",
    "                print(\"TARGET:\", t.item(), end=' ')\n",
    "                print(\"PREDICTION:\", p.item())\n",
    "            print(\"loss:\", loss)\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            print(\"acc:\",torch.sum(pred==y)/x.shape[0])\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            if idx==1: break\n",
    "            else: idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb72d6-2eb6-41d5-8d01-10a79d24436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "seeds = [31, 41, 59, 2, 65, 35, 897, 93, 2384, 626]\n",
    "#seeds = [31]\n",
    "val_result = []\n",
    "test_result = []\n",
    "for n_r in range(11):\n",
    "    print(n_r)\n",
    "    num_mod = n_r\n",
    "    val_rt = []\n",
    "    test_rt = []\n",
    "    for seed in seeds:\n",
    "        global_seed = seed\n",
    "        #replaced_dataset = dataset.map(replace_random, batched=True)\n",
    "        replaced_dataset = dataset.map(gen_adv, batched=True)\n",
    "        val_iter = preprocess(replaced_dataset['validation'])\n",
    "        test_iter = preprocess(replaced_dataset['test'])\n",
    "        val_r = evaluate(model, loss_f, val_iter, device='cuda')\n",
    "        val_rt.append(val_r)\n",
    "        test_r = evaluate(model, loss_f, test_iter, device='cuda')\n",
    "        test_rt.append(test_r)\n",
    "    val_result.append(val_rt)\n",
    "    test_result.append(test_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8c2e5-9442-42c9-a5e8-9679824fd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(11):\n",
    "    print(k)\n",
    "    avg = 0.0\n",
    "    for r in val_result[k]:\n",
    "        print(r[1].item(), end=' ')\n",
    "        avg += r[1].item()\n",
    "    print(\"AVG: \", avg/len(val_result[k]))\n",
    "print(\"=\"*80)\n",
    "for k in range(11):\n",
    "    print(k)\n",
    "    avg = 0.0\n",
    "    for r in test_result[k]:\n",
    "        print(r[1].item(), end=' ')\n",
    "        avg += r[1].item()\n",
    "    print(\"AVG: \", avg/len(val_result[k]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
