{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc871d-c9a3-471b-9442-13115e9aa615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677eac7-0fb7-42c2-bd08-0c4d336ac508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "SST = pd.read_pickle(\"./dataset/SST_train\")\n",
    "input_ids = [x for x in torch.tensor(SST[\"input_ids\"].values.tolist())]\n",
    "encoder_mask = [x for x in torch.tensor(SST[\"encoder_mask\"].values.tolist())]\n",
    "label = [x for x in torch.tensor(SST[\"label\"].values.tolist())]\n",
    "\n",
    "SST_val = pd.read_pickle(\"./dataset/SST_validation\")\n",
    "input_ids_val = [x for x in torch.tensor(SST_val[\"input_ids\"].values.tolist())]\n",
    "encoder_mask_val = [x for x in torch.tensor(SST_val[\"encoder_mask\"].values.tolist())]\n",
    "label_val = [x for x in torch.tensor(SST_val[\"label\"].values.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01439afb-0a93-4ede-a538-c766c042ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "sst_dataframe = list(zip(input_ids, encoder_mask, label))\n",
    "train_iter = DataLoader(sst_dataframe, batch_size=48, shuffle=True, num_workers=4)\n",
    "sst_val_dataframe = list(zip(input_ids_val, encoder_mask_val, label_val))\n",
    "val_iter = DataLoader(sst_val_dataframe, batch_size=48, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d588a44-b9aa-4be2-bb9a-e3796faaf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import copy\n",
    "\n",
    "# Vocab\n",
    "id_to_char = ['<PAD>', '<CLS>', '<SEP>', '<MASK>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', \\\n",
    "              '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
    "char_to_id = {c:i for i,c in enumerate(id_to_char)}\n",
    "num_spechar = 4\n",
    "vocab_size = len(id_to_char)-num_spechar\n",
    "data_max_len = 256\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_idx, max_seq_len, drop_prob):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.char_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_size)\n",
    "        self.LayerNorm = nn.LayerNorm(embed_size, eps=1e-7)\n",
    "        self.dropout = nn.Dropout(drop_prob)  # 0.1\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        position_ids = torch.arange(self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        words_embeddings = self.char_embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings  # (batchSize, sequenceLength, hidden_size)\n",
    "\n",
    "class CBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dim_feedforward, num_heads, num_layers, pad_idx):\n",
    "        super(CBERT, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, embed_size, pad_idx, data_max_len, 0.1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.prediction_layer = nn.Linear(embed_size, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        embedded_x = self.embedding_layer(x)\n",
    "        encoded_x = self.transformer_encoder(embedded_x, src_key_padding_mask=mask)\n",
    "        #return torch.flatten(encoded_x, start_dim=1)\n",
    "        #return encoded_x[:,0,:]\n",
    "        return torch.mean(encoded_x*torch.unsqueeze(mask,-1), 1, False)\n",
    "    \n",
    "class CBERT_SA(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dim_feedforward, num_heads, num_layers, pad_idx, CBERT_PATH, num_class=2):\n",
    "        super(CBERT_SA, self).__init__()\n",
    "        self.CBERT = CBERT(vocab_size, embed_size, dim_feedforward, num_heads, num_layers, pad_idx)\n",
    "        self.prediction_layer = nn.Linear(embed_size, num_class)\n",
    "        #self.prediction_layer = nn.Sequential(nn.Linear(embed_size*256, num_class))\n",
    "        #self.prediction_layer = nn.Sequential(nn.Linear(embed_size, 30), nn.ReLU(), nn.Linear(30, num_class))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # init\n",
    "        checkpoint = torch.load(CBERT_PATH)\n",
    "        self.CBERT.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        self.prediction_layer.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        #print(x.shape)\n",
    "        encoded_x = self.CBERT(x, mask)\n",
    "        #print(encoded_x.shape)\n",
    "        pred = self.prediction_layer(encoded_x)\n",
    "        #print(pred.shape)\n",
    "        ret = self.softmax(pred)\n",
    "        #print(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e29e86-9413-4080-943f-6655d9d80d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBERT_SA(vocab_size=len(id_to_char),\n",
    "                 embed_size=768,\n",
    "                 dim_feedforward=2048,\n",
    "                 num_heads=12,\n",
    "                 num_layers=6,\n",
    "                 pad_idx=char_to_id['<PAD>'],\n",
    "                 CBERT_PATH = \"./models/yelp6_cp12.pt\")\n",
    "model = model.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202890a-ac78-46c3-bb12-2c71aa91fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, loss_f, train_iter, num_epochs, device='cpu', prnt_intv=1):\n",
    "    model = model.to(device=device)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_sum = torch.tensor([0.0], device=device)\n",
    "        train_acc_sum = torch.tensor([0.0], device=device)\n",
    "        n = 0\n",
    "        #for x, mask, y in tqdm(train_iter):\n",
    "        for x, mask, y in train_iter:\n",
    "            x, mask, y = x.to(device=device), mask.to(device=device), y.to(device=device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, mask)\n",
    "            \n",
    "            loss = loss_f(y_hat, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5e-5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #if not torch.isfinite(loss):\n",
    "            #    print(x, y, iid, mask)\n",
    "            with torch.no_grad():\n",
    "                train_loss_sum += loss.float()\n",
    "                pred = torch.argmax(y_hat, dim=1)\n",
    "                train_acc_sum += torch.sum(pred==y)\n",
    "                n += x.shape[0]\n",
    "                    \n",
    "        if (epoch+1)%prnt_intv == 0:\n",
    "            print(\"Epoch:%d Loss:%f, TrainAcc:%f\"%(epoch+1,train_loss_sum/n,train_acc_sum/n))\n",
    "            \n",
    "def evaluate(model, loss_f, val_iter, device='cpu'):\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum = torch.tensor([0.0], device=device)\n",
    "        acc_sum = torch.tensor([0.0], device=device)\n",
    "        n = 0\n",
    "        \n",
    "        #for x, mask, y in tqdm(val_iter):\n",
    "        for x, mask, y in val_iter:\n",
    "            x, mask, y = x.to(device=device), mask.to(device=device), y.to(device=device)\n",
    "            y_hat = model(x, mask)            \n",
    "            loss = loss_f(y_hat, y)\n",
    "        \n",
    "            loss_sum += loss.float()\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            acc_sum += torch.sum(pred==y)\n",
    "            n += x.shape[0]\n",
    "        \n",
    "    print(\"Val_Loss:%f, Val_Acc:%f\"%(loss_sum/n, acc_sum/n))\n",
    "\n",
    "def print_decoded_ids(ids):\n",
    "    for c in ids:\n",
    "        if c==0: # <PAD>, <SEP>\n",
    "            break\n",
    "        else:\n",
    "            print(id_to_char[c], end='')\n",
    "    print()\n",
    "    \n",
    "def show_sample(model, data_iter, loss_f, device='cpu'):\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        idx=0\n",
    "        for x, mask, y in data_iter:\n",
    "            x, mask, y = x.to(device=device), mask.to(device=device), y.to(device=device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, mask)            \n",
    "            loss = loss_f(y_hat, y)\n",
    "\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            for i,p,t in zip(x, pred, y):\n",
    "                print(\"INPUT:\", end=' ')\n",
    "                print_decoded_ids(i)\n",
    "                print(\"TARGET:\", t.item(), end=' ')\n",
    "                print(\"PREDICTION:\", p.item())\n",
    "            print(\"loss:\", loss)\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            print(\"acc:\",torch.sum(pred==y)/x.shape[0])\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            if idx==10: break\n",
    "            else: idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae446ef-c0c9-482b-8e93-e1c521d7f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=0.0)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "for i in range(5):\n",
    "    train(model, optimizer, loss_f, train_iter, num_epochs=1, device='cuda', prnt_intv=1)\n",
    "    evaluate(model, loss_f, val_iter, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4561ef2-6bc0-4a2d-9bbb-6060e1b3250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, loss_f, val_iter, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f7e96-6fcc-4772-a7fd-988272734365",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(model, val_iter, loss_f, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895f97b-072c-4cfe-8dd2-1bb5d7d285e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./models/SST_sample.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict' : model.state_dict(),\n",
    "}, PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
